{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wMrwqRLs91Vf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.util import ngrams\n",
    "#from google.colab import drive\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>stance</th>\n",
       "      <th>tweet</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CI_ESRX</td>\n",
       "      <td>support</td>\n",
       "      <td>Cigna and ESI set to merge. Here we go...</td>\n",
       "      <td>healthcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CI_ESRX</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Express Scripts Closes Acquisition Of eviCore;...</td>\n",
       "      <td>healthcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CI_ESRX</td>\n",
       "      <td>comment</td>\n",
       "      <td>RT @_diginsurance: Cigna-Express Scripts deal ...</td>\n",
       "      <td>healthcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CI_ESRX</td>\n",
       "      <td>support</td>\n",
       "      <td>Here's the just-released 400+ page merger prox...</td>\n",
       "      <td>healthcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CI_ESRX</td>\n",
       "      <td>support</td>\n",
       "      <td>Cigna nears deal for Express Scripts https://t...</td>\n",
       "      <td>healthcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44318</th>\n",
       "      <td>FOXA_DIS</td>\n",
       "      <td>comment</td>\n",
       "      <td>@Almightyvonzzus @Lampshadej Nah Disney bought...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44319</th>\n",
       "      <td>FOXA_DIS</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>what if the way Marvel/Disney brings in Fox’s ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44320</th>\n",
       "      <td>FOXA_DIS</td>\n",
       "      <td>comment</td>\n",
       "      <td>#Deadpool creator Rob Liefeld is not in favour...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44321</th>\n",
       "      <td>FOXA_DIS</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Hey @Disney, I could use a buyout.</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44322</th>\n",
       "      <td>FOXA_DIS</td>\n",
       "      <td>comment</td>\n",
       "      <td>Disney Not Likely To Disney-Up Fox Properties ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44323 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target     stance                                              tweet  \\\n",
       "0       CI_ESRX    support          Cigna and ESI set to merge. Here we go...   \n",
       "1       CI_ESRX  unrelated  Express Scripts Closes Acquisition Of eviCore;...   \n",
       "2       CI_ESRX    comment  RT @_diginsurance: Cigna-Express Scripts deal ...   \n",
       "3       CI_ESRX    support  Here's the just-released 400+ page merger prox...   \n",
       "4       CI_ESRX    support  Cigna nears deal for Express Scripts https://t...   \n",
       "...         ...        ...                                                ...   \n",
       "44318  FOXA_DIS    comment  @Almightyvonzzus @Lampshadej Nah Disney bought...   \n",
       "44319  FOXA_DIS  unrelated  what if the way Marvel/Disney brings in Fox’s ...   \n",
       "44320  FOXA_DIS    comment  #Deadpool creator Rob Liefeld is not in favour...   \n",
       "44321  FOXA_DIS  unrelated                 Hey @Disney, I could use a buyout.   \n",
       "44322  FOXA_DIS    comment  Disney Not Likely To Disney-Up Fox Properties ...   \n",
       "\n",
       "              domain  \n",
       "0         healthcare  \n",
       "1         healthcare  \n",
       "2         healthcare  \n",
       "3         healthcare  \n",
       "4         healthcare  \n",
       "...              ...  \n",
       "44318  entertainment  \n",
       "44319  entertainment  \n",
       "44320  entertainment  \n",
       "44321  entertainment  \n",
       "44322  entertainment  \n",
       "\n",
       "[44323 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_hlt_train = pd.read_csv(\"/data/parush/wtwt/healthcare_train.txt\", sep='\\t')\n",
    "# df_hlt_test = pd.read_csv(\"/data/parush/wtwt/healthcare_test.txt\", sep='\\t')\n",
    "# df_ent_train = pd.read_csv(\"/data/parush/wtwt/entertainment_train.txt\", sep='\\t')\n",
    "# df_ent_test = pd.read_csv(\"/data/parush/wtwt/entertainment_test.txt\", sep='\\t')\n",
    "# print(\"Length of Health_train\", len(df_hlt_train))\n",
    "# print(\"Length of Health_test\", len(df_hlt_test))\n",
    "# print(\"Length of ent_train\", len(df_ent_train))\n",
    "# print(\"Length of ent_test\", len(df_ent_test))\n",
    "file =open('/data/parush/wtwt/wtwt_extracted.json','r')\n",
    "data = json.load(file)\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop(columns = 'tweet_id')\n",
    "df = df.rename(columns={\"merger\": \"target\"})\n",
    "df['domain'] = np.where(df['target']== 'FOXA_DIS', 'entertainment', 'healthcare')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'support':0, 'refute': 1, 'comment': 2, 'unrelated': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = 'tfidf'   # set 'count' or 'tfidf'\n",
    "analyzer = 'both'  # set 'word' or 'both' ( word and char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cu37tw9Vmm3p",
    "outputId": "497cfeae-f6d3-4588-b465-80ece0117023"
   },
   "outputs": [],
   "source": [
    "if vectorizer == 'count':\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = CountVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='word',ngram_range=(1,3))\n",
    "        char_vectorizer = CountVectorizer(analyzer='char',ngram_range=(2,5))\n",
    "else:\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,3))\n",
    "        char_vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(2,5))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "L5QKhyyn3GW3"
   },
   "outputs": [],
   "source": [
    "def in_splitter(df, domain, classes):\n",
    "    print(\"Started splitting {}, pre-processing\".format(domain))\n",
    "    df = df[df['domain'] == domain]\n",
    "    X = df[['target','tweet', 'domain']]\n",
    "    y = df[['stance']] \n",
    "    X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.25, random_state=42, shuffle=True,stratify = y)\n",
    "    \n",
    "    train_corpus = X_train_['tweet'].tolist()\n",
    "    train_labels = [classes[i] for i in y_train_['stance'].tolist()]\n",
    "    test_corpus = X_test_['tweet'].tolist()\n",
    "    test_labels = [classes[i] for i in y_test_['stance'].tolist()]\n",
    "    \n",
    "    if analyzer == 'word':\n",
    "        ngram_vectorized_data = vectorizer.fit_transform(train_corpus)\n",
    "        test_ngram_vectorized_data = vectorizer.transform(test_corpus)\n",
    "        return ngram_vectorized_data, train_labels, test_ngram_vectorized_data, test_labels\n",
    "    else:\n",
    "        ngram_vectorized_data = vectorizer.fit_transform(train_corpus)\n",
    "        char_vectorized_data = char_vectorizer.fit_transform(train_corpus)\n",
    "        l = np.hstack((ngram_vectorized_data.toarray(), char_vectorized_data.toarray()))\n",
    "        train_vectorized_data = sparse.csr_matrix(l)\n",
    "     \n",
    "        test_ngram_vectorized_data = vectorizer.transform(test_corpus)\n",
    "        test_char_vectorized_data = char_vectorizer.transform(test_corpus)\n",
    "        l2 = np.hstack((test_ngram_vectorized_data.toarray(), test_char_vectorized_data.toarray()))\n",
    "        test_vectorized_data = sparse.csr_matrix(l2)\n",
    "        \n",
    "        return train_vectorized_data, train_labels, test_vectorized_data,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgrH2FsLNu8B",
    "outputId": "577a1218-22f0-4183-b362-09a027f2fe2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started splitting healthcare, pre-processing\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 64.6 GiB for an array with shape (22101, 392049) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49540/2808164326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdomains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'ent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'entertainment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hlt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'healthcare'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0min_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hlt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_49540/2722012297.py\u001b[0m in \u001b[0;36min_splitter\u001b[0;34m(df, domain, classes)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mngram_vectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mchar_vectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_vectorized_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_vectorized_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_vectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.9/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.9/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 64.6 GiB for an array with shape (22101, 392049) and data type float64"
     ]
    }
   ],
   "source": [
    "domains = {'ent': 'entertainment', 'hlt':'healthcare'}\n",
    "X_train, y_train, X_test, y_test =  in_splitter(df, domains['hlt'],classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3559, 1: 2598, 2: 8346, 3: 7598}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique,count = np.unique(y_train,return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22101"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1187, 1: 866, 2: 2782, 3: 2532}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique,count = np.unique(y_test,return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tR4ULhqpQR_h",
    "outputId": "5866db2f-b8f4-46a1-a353-8a1ac3a616db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.094 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.094 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.847 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.094 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.823 (+/-0.008) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.847 (+/-0.001) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.803 (+/-0.001) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.823 (+/-0.008) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.806 (+/-0.001) for {'C': 1, 'kernel': 'linear'}\n",
      "0.802 (+/-0.002) for {'C': 10, 'kernel': 'linear'}\n",
      "0.802 (+/-0.002) for {'C': 100, 'kernel': 'linear'}\n",
      "0.802 (+/-0.002) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8980    0.1929    0.3176      1187\n",
      "           1     0.9404    0.1640    0.2793       866\n",
      "           2     0.5516    0.9137    0.6880      2782\n",
      "           3     0.8504    0.7903    0.8192      2532\n",
      "\n",
      "    accuracy                         0.6670      7367\n",
      "   macro avg     0.8101    0.5152    0.5260      7367\n",
      "weighted avg     0.7558    0.6670    0.6254      7367\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.250 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.250 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.401 (+/-0.001) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.250 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.701 (+/-0.001) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.402 (+/-0.001) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.764 (+/-0.002) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.701 (+/-0.001) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.766 (+/-0.003) for {'C': 1, 'kernel': 'linear'}\n",
      "0.763 (+/-0.003) for {'C': 10, 'kernel': 'linear'}\n",
      "0.763 (+/-0.003) for {'C': 100, 'kernel': 'linear'}\n",
      "0.763 (+/-0.003) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7755    0.7801    0.7778      1187\n",
      "           1     0.7826    0.7691    0.7758       866\n",
      "           2     0.7605    0.8102    0.7845      2782\n",
      "           3     0.9059    0.8436    0.8736      2532\n",
      "\n",
      "    accuracy                         0.8120      7367\n",
      "   macro avg     0.8061    0.8007    0.8029      7367\n",
      "weighted avg     0.8155    0.8120    0.8130      7367\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "\n",
    "for score in scores:\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state = 2 )\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score, cv = cv\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred, digits = 4,))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7755    0.7801    0.7778      1187\n",
      "           1     0.7826    0.7691    0.7758       866\n",
      "           2     0.7605    0.8102    0.7845      2782\n",
      "           3     0.9059    0.8436    0.8736      2532\n",
      "\n",
      "    accuracy                         0.8120      7367\n",
      "   macro avg     0.8061    0.8007    0.8029      7367\n",
      "weighted avg     0.8155    0.8120    0.8130      7367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, digits = 4,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPmjri5VbvZP"
   },
   "source": [
    "# Test on other Target\n",
    "False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryWZpF-zxohF",
    "outputId": "54ddc458-facf-4811-cf86-7aa438113765"
   },
   "outputs": [],
   "source": [
    "X_test_, y_test_ = get_test_data_and_labels(test_data_file_m,TARGETS_m[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCebQWZuqO3U",
    "outputId": "b36a171a-b1b7-4737-e616-887c412aa8fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2414    0.4565    0.3158        46\n",
      "           1     0.6622    0.5185    0.5816       189\n",
      "\n",
      "   micro avg     0.5064    0.5064    0.5064       235\n",
      "   macro avg     0.4518    0.4875    0.4487       235\n",
      "weighted avg     0.5798    0.5064    0.5296       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true_, y_pred_ = y_test_, clf.predict(X_test_)\n",
    "print(classification_report(y_true_, y_pred_, digits = 4, labels = [0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tkt-iBNjQ2WA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhFyOPDNaKFW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrqI0RAdxeSs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKwr73fHxe_G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Count_Stance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlpKernel",
   "language": "python",
   "name": "nlpkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
