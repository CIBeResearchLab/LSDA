{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "wMrwqRLs91Vf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.util import ngrams\n",
    "#from google.colab import drive\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_m = '/data/parush/stance_mohammed/new_train.txt'\n",
    "test_data_file_m = '/data/parush/stance_mohammed/new_test.txt'\n",
    "TARGETS_m = [ 'Atheism','Climate Change is a Real Concern', 'Feminist Movement','Hillary Clinton', 'Legalization of Abortion', 'Donald Trump']\n",
    "\n",
    "\n",
    "# train_data_file_s = '/data/parush/SomasundaranWiebe-politicalDebates/train.txt'\n",
    "# test_data_file_s = '/data/parush/SomasundaranWiebe-politicalDebates/test.txt'\n",
    "# TARGETS_s = ['god','healthcare','guns','gayRights','abortion', 'creation']\n",
    "\n",
    "\n",
    "# train_data_file_q = '/data/parush/Data_MPCHI/train.txt'\n",
    "# test_data_file_q = '/data/parush/Data_MPCHI/test.txt'\n",
    "# TARGETS_q = ['Are E-Cigarettes safe?','Does MMR Vaccine lead to autism in children?',\n",
    "#       'Does Sunlight exposure lead to skin cancer?','Does Vitamin C prevent common cold?',\n",
    "#       'Should women take HRT post-menopause?']\n",
    "# # Loading Data\n",
    "# df = pd.read_csv(train_data_file, sep='\\t')\n",
    "# df1 = pd.read_csv(test_data_file, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "# #Use below lines only when training different classifiers for different targerts and testing on their corrosponding data.\n",
    "# # t = ['god','healthcare','guns','gayRights','abortion', 'creation']\n",
    "# # df = df[(df[\"Target\"]==t[1]) ]\n",
    "# # df1 = df1[(df1[\"Target\"]==t[1]) ]\n",
    "\n",
    "# print(\"The length of train data is {}\".format(len(df)))\n",
    "# print(\"The length of test data is {}\".format(len(df1)))\n",
    "\n",
    "# #dummy\n",
    "# import numpy as np\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "# X = y_train\n",
    "# y = y_test\n",
    "\n",
    "# dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "# dummy_clf.fit(X,X)\n",
    "\n",
    "\n",
    "# y_true, y_pred = y_test, dummy_clf.predict(y)\n",
    "# print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = 'tfidf'   # set 'count' or 'tfidf'\n",
    "analyzer = 'both'  # set 'word' or 'both' ( word and char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cu37tw9Vmm3p",
    "outputId": "497cfeae-f6d3-4588-b465-80ece0117023"
   },
   "outputs": [],
   "source": [
    "if vectorizer == 'count':\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = CountVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='word',ngram_range=(1,3))\n",
    "        char_vectorizer = CountVectorizer(analyzer='char',ngram_range=(2,5))\n",
    "else:\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,3))\n",
    "        char_vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(2,5))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "id": "pqbMqivk-EBF"
   },
   "outputs": [],
   "source": [
    "#List of FAVOR Tweets\n",
    "def get_training_data_and_labels(file,target):\n",
    "    train_corpus = []\n",
    "    train_labels = []\n",
    "    classes = {'FAVOR': 0, 'AGAINST': 1, 'NONE': 2}\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.replace('#SemST', '').strip()\n",
    "            line = line.split('\\t')\n",
    "            if line[0].strip() != 'ID' and target in line[1].strip():\n",
    "            #if line[0].strip() != 'ID':\n",
    "                \n",
    "                tweet = line[2]\n",
    "                stance = line[3]\n",
    "                \n",
    "                train_corpus.append(tweet)\n",
    "                train_labels.append(classes[stance])\n",
    "    c_l = len(train_corpus)\n",
    "    print(\"Current length\",c_l)\n",
    "    if aug:\n",
    "        with open('rich_data/dt_ath_hc/Donald Trump_Atheism_Hillary Clinton0.9.txt','r') as newfile:\n",
    "            for line in newfile:\n",
    "                line = line.replace('#SemST', '').strip()\n",
    "                line = line.split('\\t')\n",
    "                tweet = line[0]\n",
    "                stance = line[1]\n",
    "                #tweet = process_tweet(tweet)\n",
    "                train_corpus.append(tweet)\n",
    "                train_labels.append(classes[stance.strip()])\n",
    "    print(\"Added more labels\", len(train_corpus) - c_l )\n",
    "    \n",
    "    if analyzer == 'word':\n",
    "        ngram_vectorized_data = vectorizer.fit_transform(train_corpus)\n",
    "        \n",
    "        return ngram_vectorized_data, train_labels\n",
    "    else:\n",
    "        ngram_vectorized_data = vectorizer.fit_transform(train_corpus)\n",
    "        char_vectorized_data = char_vectorizer.fit_transform(train_corpus)\n",
    "        l = np.hstack((ngram_vectorized_data.toarray(), char_vectorized_data.toarray()))\n",
    "        train_vectorized_data = sparse.csr_matrix(l)\n",
    "        \n",
    "        return train_vectorized_data, train_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2QL4bjHgloM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "id": "rKNZ6AA4G9Tv"
   },
   "outputs": [],
   "source": [
    "#preparing test_data\n",
    "def get_test_data_and_labels(file,target):\n",
    "    test_corpus = []\n",
    "    test_labels = []\n",
    "    classes = {'FAVOR': 0, 'AGAINST': 1, 'NONE': 2}\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.replace('#SemST', '').strip()\n",
    "            line = line.split('\\t')\n",
    "        #if line[0].strip() != 'ID' and line[3].strip() == 'FAVOR' and line[1] == t:\n",
    "            if line[0].strip() != 'ID' and target in line[1].strip():\n",
    "                tweet = line[2]\n",
    "                stance = line[3]\n",
    "                #print(line[0])\n",
    "                test_corpus.append(tweet)\n",
    "                test_labels.append(classes[stance])\n",
    "    if analyzer == 'word':\n",
    "        test_ngram_vectorized_data = vectorizer.transform(test_corpus)\n",
    "        \n",
    "        return test_ngram_vectorized_data, test_labels\n",
    "    else:\n",
    "        test_ngram_vectorized_data = vectorizer.transform(test_corpus)\n",
    "        test_char_vectorized_data = char_vectorizer.transform(test_corpus)\n",
    "        l2 = np.hstack((test_ngram_vectorized_data.toarray(), test_char_vectorized_data.toarray()))\n",
    "        test_vectorized_data = sparse.csr_matrix(l2)\n",
    "        \n",
    "        \n",
    "        return test_vectorized_data,test_labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBQR_GZS-Zwd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgrH2FsLNu8B",
    "outputId": "577a1218-22f0-4183-b362-09a027f2fe2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current length 653\n",
      "Added more labels 248\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train =  get_training_data_and_labels(train_data_file_m, TARGETS_m[4])\n",
    "X_test, y_test = get_test_data_and_labels(test_data_file_m,TARGETS_m[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 45, 1: 172, 2: 78}"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique,count = np.unique(y_train,return_counts=True)\n",
    "dict(zip(unique, count))\n",
    "unique,count = np.unique(y_test,return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tR4ULhqpQR_h",
    "outputId": "5866db2f-b8f4-46a1-a353-8a1ac3a616db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.192 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.192 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.192 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.192 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.192 (+/-0.000) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.192 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.686 (+/-0.020) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.192 (+/-0.000) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.689 (+/-0.014) for {'C': 1, 'kernel': 'linear'}\n",
      "0.670 (+/-0.052) for {'C': 10, 'kernel': 'linear'}\n",
      "0.670 (+/-0.052) for {'C': 100, 'kernel': 'linear'}\n",
      "0.670 (+/-0.052) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4444    0.0889    0.1481        45\n",
      "           1     0.6527    0.9070    0.7591       172\n",
      "\n",
      "   micro avg     0.6452    0.7373    0.6882       217\n",
      "   macro avg     0.5486    0.4979    0.4536       217\n",
      "weighted avg     0.6095    0.7373    0.6324       217\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.333 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.333 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.333 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.333 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.333 (+/-0.000) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.333 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.434 (+/-0.046) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.333 (+/-0.000) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.436 (+/-0.043) for {'C': 1, 'kernel': 'linear'}\n",
      "0.432 (+/-0.050) for {'C': 10, 'kernel': 'linear'}\n",
      "0.432 (+/-0.050) for {'C': 100, 'kernel': 'linear'}\n",
      "0.432 (+/-0.050) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4444    0.0889    0.1481        45\n",
      "           1     0.6527    0.9070    0.7591       172\n",
      "\n",
      "   micro avg     0.6452    0.7373    0.6882       217\n",
      "   macro avg     0.5486    0.4979    0.4536       217\n",
      "weighted avg     0.6095    0.7373    0.6324       217\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "\n",
    "for score in scores:\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state = 2 )\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score, cv = cv\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred, digits = 4, labels = [0,1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{90: 45.36, 80: 48.67, 70: 46.69, 60: 56.03, 50: 52.33}"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{90:45.36 , 80:48.67, 70:46.69, 60:56.03, 50:52.33}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{90: , 80:, 70:, 60:, 50:}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPmjri5VbvZP"
   },
   "source": [
    "# Test on other Target\n",
    "False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryWZpF-zxohF",
    "outputId": "54ddc458-facf-4811-cf86-7aa438113765"
   },
   "outputs": [],
   "source": [
    "X_test_, y_test_ = get_test_data_and_labels(test_data_file_m,TARGETS_m[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCebQWZuqO3U",
    "outputId": "b36a171a-b1b7-4737-e616-887c412aa8fa"
   },
   "outputs": [],
   "source": [
    "y_true_, y_pred_ = y_test_, clf.predict(X_test_)\n",
    "print(classification_report(y_true_, y_pred_, digits = 4, labels = [0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tkt-iBNjQ2WA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhFyOPDNaKFW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrqI0RAdxeSs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKwr73fHxe_G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Count_Stance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlpKernel",
   "language": "python",
   "name": "nlpkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
