{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47c95f5-c1ba-4a45-bf43-917fd3c30b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples \n",
    "from stance_utils import *\n",
    "#from parameters import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Dropout,Concatenate,Dense, Embedding, SpatialDropout1D, Flatten, GRU, Bidirectional, Conv1D,MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.layers import RNN, Dropout,Concatenate,Dense, Embedding,LSTMCell, LSTM, SpatialDropout1D, Flatten, GRU, Bidirectional, Conv1D, Input,MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "stopwords_english = stopwords.words('english')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1eb2ac-3c04-4226-ad5f-ae132b6fae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'FAVOR': np.array([1, 0, 0]), 'AGAINST': np.array([0, 1, 0]), 'NONE': np.array([0, 0, 1])}\n",
    "classes_ = np.array(['FAVOR', 'AGAINST', 'NONE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc10ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65230d80-7c1f-453d-a66c-c5043bf88588",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_m = '/data/parush/stance_mohammed/train.txt'\n",
    "test_data_file_m = '/data/parush/stance_mohammed/test.txt'\n",
    "TARGETS_m = [ 'Atheism','Climate Change is a Real Concern', 'Feminist Movement','Hillary Clinton', 'Legalization of Abortion' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f08aad6-d353-4f23-82d6-034c785aaff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(train_file, test_file, target):\n",
    "    \n",
    "    sentence_maxlen = 0\n",
    "    target_maxlen = 0\n",
    "    x_s_token = []\n",
    "    x_t_token = []\n",
    "    y_train = []\n",
    "\n",
    "    \n",
    "    with open(train_file, 'r') as trainfile:\n",
    "        for line in trainfile: \n",
    "            line = line.replace('#SemST', '').strip()\n",
    "            line = line.split('\\t')\n",
    "            \n",
    "            \n",
    "            if line[0].strip() != 'ID' and target in line[1].strip():\n",
    "                tweet = line[2]\n",
    "                tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                x_s_token.append(tweet)\n",
    "                target_ = line[1].strip().lower().split()\n",
    "                if len(target_) > target_maxlen:\n",
    "                    target_maxlen = len(target_)\n",
    "                x_t_token.append(target_)\n",
    "                y_train.append(classes[line[3].strip()])\n",
    "    \n",
    "\n",
    "                               \n",
    "    \n",
    "    x_s_test_token = []\n",
    "    x_t_test_token = []\n",
    "    y_test = []\n",
    "    with open(test_file, 'r') as testfile:\n",
    "        for line in testfile:\n",
    "            line = line.replace('#SemST', '').strip()\n",
    "            line = line.split('\\t')\n",
    "        \n",
    "\n",
    "            \n",
    "            if line[0] != 'ID' and target in line[1].strip():\n",
    "                tweet = line[2]\n",
    "                tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                x_s_test_token.append(tweet)\n",
    "                target_ = line[1].strip().lower().split()\n",
    "                if len(target_) > target_maxlen:\n",
    "                    target_maxlen = len(target_)\n",
    "                x_t_test_token.append(target_)\n",
    "                y_test.append(classes[line[3].strip()])\n",
    "\n",
    "\n",
    "    \n",
    "    return x_s_token, x_t_token, x_s_test_token, x_t_test_token, y_train, y_test, sentence_maxlen, target_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ddd7e6-fbc1-4815-9486-7ec4fd80ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s_token, x_t_token, x_s_test_token, x_t_test_token, y_train, y_test, sentence_maxlen, target_maxlen  = train_and_test(train_data_file_m, test_data_file_m, TARGETS_m[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5668309-53e2-466c-b413-0df0aac2eecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 2943\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_vocab(x_s_token + x_t_token + x_s_test_token + x_t_test_token)\n",
    "vocab_size = len(vocabulary)\n",
    "print(\"Total words in vocab are\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8215158-4421-4a4c-88c0-b60598dbf465",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = [tweet_to_tensor(each_s,vocabulary) for each_s in x_s_token]\n",
    "x_s = pad_sequences(x_s, maxlen = sentence_maxlen, padding = 'post')\n",
    "x_s_test = [tweet_to_tensor(each_s,vocabulary) for each_s in x_s_test_token]\n",
    "x_s_test = pad_sequences(x_s_test, maxlen = sentence_maxlen, padding = 'post')\n",
    "\n",
    "x_t = [tweet_to_tensor(each_s,vocabulary) for each_s in x_t_token]\n",
    "x_t = pad_sequences(x_t, maxlen = sentence_maxlen, padding = 'post')\n",
    "x_t_test = [tweet_to_tensor(each_s,vocabulary) for each_s in x_t_test_token]\n",
    "x_t_test = pad_sequences(x_t_test, maxlen = sentence_maxlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b255d44e-796b-498e-9736-ecf96a732ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "x_s = x_s[shuffle_indices]\n",
    "x_t = x_t[shuffle_indices]\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_train = y_train[shuffle_indices]\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29175129",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = x_s[:640]\n",
    "x_t = x_t[:640]\n",
    "y_train= y_train[:640]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef1fa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t =np.array( [x.reshape(1,18) for x in x_t[:640]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aafc8882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 16:26:34.961295: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/cuda/cuda-11.2/lib64:/slurm/include\n",
      "2021-10-15 16:26:34.961336: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-15 16:26:34.961363: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gaivi.cse.usf.edu): /proc/driver/nvidia/version does not exist\n",
      "2021-10-15 16:26:34.961985: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "x_t = tf.convert_to_tensor(\n",
    "    x_t, dtype=tf.float32, dtype_hint=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ae0b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = tf.convert_to_tensor(np.zeros([16, 128]).astype(np.float32))\n",
    "c_0 = tf.convert_to_tensor(np.zeros([16, 128]).astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4d0fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_states = [h_0, c_0]\n",
    "# encoder = RNN(tf.compat.v1.nn.rnn_cell.LSTMCell(128), return_state= True)\n",
    "# output, c_0, h_0 = encoder(x_t[:16],initial_state = i_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac6aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2b7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae66791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7557d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85ba679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = x_t[:16]\n",
    "lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "whole_seq_output, h_0, c_0 = lstm(inputs,initial_state = [h_0, c_0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306b65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a14e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = tf.convert_to_tensor(np.zeros([16, 128]).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d6ae32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix = tf.random.uniform([vocab_size, 300], -0.1, 0.1) #input_size is embeddings size\n",
    "embedding_matrix = get_embeddings('wikipedia',300,vocabulary)                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90467299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2943, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47ee7754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 18)]              0         \n",
      "_________________________________________________________________\n",
      "Embedding (Embedding)        (None, 18, 300)           882900    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (16, 128)                 219648    \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (16, 128)                 0         \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (16, 3)                   387       \n",
      "=================================================================\n",
      "Total params: 1,102,935\n",
      "Trainable params: 1,102,935\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs2 = Input(shape=(sentence_maxlen), name = 'Input')\n",
    "\n",
    "embedded_inputs = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], name = 'Embedding')(inputs2)\n",
    "embedded_inputs2 = Dropout(0.2)(embedded_inputs)\n",
    "lstm = LSTM(128, activation = 'tanh',dropout=0.3,name = 'lstm')(embedded_inputs2, initial_state = [h_0, c_0])\n",
    "flat = Flatten(name = 'Flatten')(lstm)\n",
    "output = (Dense(3,activation='softmax',name = 'Dense'))(flat)\n",
    "model2 = Model(inputs=inputs2, outputs=output)\n",
    "model2.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])    \n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac6492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ff376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26fc23fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 [==============================] - 2s 17ms/step - loss: 1.0211 - accuracy: 0.4953\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.9002 - accuracy: 0.5859\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.7107 - accuracy: 0.7000\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.5136 - accuracy: 0.7984\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.3200 - accuracy: 0.8781\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.1903 - accuracy: 0.9391\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.2363 - accuracy: 0.9187\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.1080 - accuracy: 0.9641\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0890 - accuracy: 0.9688\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0709 - accuracy: 0.9828\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0732 - accuracy: 0.9734\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0527 - accuracy: 0.9859\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0445 - accuracy: 0.9828\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0212 - accuracy: 0.9922\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0343 - accuracy: 0.9906\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0501 - accuracy: 0.9859\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0214 - accuracy: 0.9953\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0433 - accuracy: 0.9891\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0336 - accuracy: 0.9922\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0281 - accuracy: 0.9906\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0258 - accuracy: 0.9922\n",
      "Epoch 22/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0429 - accuracy: 0.9875\n",
      "Epoch 23/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0093 - accuracy: 0.9969\n",
      "Epoch 24/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0077 - accuracy: 0.9984\n",
      "Epoch 25/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0056 - accuracy: 0.9969\n",
      "Epoch 26/50\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.0836 - accuracy: 0.9766\n",
      "Epoch 27/50\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.0267 - accuracy: 0.9922\n",
      "Epoch 28/50\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.0277 - accuracy: 0.9937\n",
      "Epoch 29/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0231 - accuracy: 0.9922\n",
      "Epoch 30/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0238 - accuracy: 0.9937\n",
      "Epoch 31/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0071 - accuracy: 0.9984\n",
      "Epoch 34/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0113 - accuracy: 0.9953\n",
      "Epoch 36/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0097 - accuracy: 0.9953\n",
      "Epoch 37/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 6.1624e-04 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0070 - accuracy: 0.9953\n",
      "Epoch 40/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0168 - accuracy: 0.9953\n",
      "Epoch 42/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0073 - accuracy: 0.9969\n",
      "Epoch 43/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0206 - accuracy: 0.9953\n",
      "Epoch 44/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 5.2375e-04 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0159 - accuracy: 0.9953\n",
      "Epoch 47/50\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.0200 - accuracy: 0.9922\n",
      "Epoch 48/50\n",
      "40/40 [==============================] - 1s 17ms/step - loss: 0.0154 - accuracy: 0.9969\n",
      "Epoch 49/50\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.0155 - accuracy: 0.9969\n",
      "Epoch 50/50\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.0199 - accuracy: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d4c0e0790>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_s, y_train, epochs = 50, batch_size = 16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1599c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd4b38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(model2.predict(x_s_test[:272], batch_size = 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72601de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97303229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3077    0.4000    0.3478        50\n",
      "           1     0.7931    0.6389    0.7077       180\n",
      "\n",
      "   micro avg     0.6429    0.5870    0.6136       230\n",
      "   macro avg     0.5504    0.5194    0.5278       230\n",
      "weighted avg     0.6876    0.5870    0.6295       230\n",
      " samples avg     0.4963    0.4963    0.4963       230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p/parush/.conda/envs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.conda/envs/nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test[:272], y_pred, digits=4, labels = [0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9bf70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "# cvscores = []\n",
    "\n",
    "# for train, val in kfold.split(x_s, classes_[y_train.argmax(1)]): \n",
    "#     model2.fit(x_s[train], y_train[train], epochs = 5, batch_size = 16, verbose=1)\n",
    "# #     scores = model2.evaluate(x_s[val], y_train[val], verbose=0)\n",
    "# #     print(\"%s: %.2f%%\" % (model2.metrics_names[1], scores[1]*100))\n",
    "# #     cvscores.append(scores[1] * 100)\n",
    "# # print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c3ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d99275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215cf50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7b8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1e8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52942f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70803a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_weights = get_embeddings('wikipedia',300,vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3085bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have to do this?\n",
    "# x_s_ = tf.convert_to_tensor(\n",
    "#     x_s_, dtype=tf.float32, dtype_hint=None, name=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc757d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# j = 16\n",
    "# while j<=640:\n",
    "#     inputs = x_t[i:j]\n",
    "#     lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "#     whole_seq_output, h_0, c_0 = lstm(inputs,initial_state = [h_0, c_0])\n",
    "#     i += 16\n",
    "#     j += 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478352ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = Input(batch_shape=(16,1,18))\n",
    "# lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "# whole_seq_output, h_0, c_0 = lstm(inputs)\n",
    "# model = Model(inputs=inputs, outputs=whole_seq_output)\n",
    "# k = model.predict(x_t[:640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_memory_state = tf.reshape(\n",
    "#     final_memory_state, (16,1,128), name=None\n",
    "# )\n",
    "\n",
    "# final_carry_state = tf.reshape(\n",
    "#     final_carry_state, (16,1,128), name=None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inputs2 = Input(shape=(1,sentence_maxlen,), name = 'Input')\n",
    "# #embedded_inputs = Embedding(embeddings_weights.shape[0], embeddings_weights.shape[1], weights=[embeddings_weights], name = 'Embedding')(inputs2)\n",
    "# #embedded_inputs2 = Dropout(0.2)(embedded_inputs)\n",
    "# lstm, s1,s2 = LSTM(128,return_sequences=True,return_state=True,dropout=0.3,name = 'lstm')(inputs2, initial_state = i_states)\n",
    "# #lstm = LSTM(128,return_sequences=True,dropout=0.3,name = 'lstm')(inputs2)\n",
    "# flat = Flatten(name = 'Flatten')(lstm)\n",
    "# output = (Dense(3,activation='softmax',name = 'Dense'))(flat)\n",
    "# model2 = Model(inputs=inputs2, outputs=output)\n",
    "# model2.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])    \n",
    "# model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "59727728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594aaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd77a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71ef3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "42a83d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f918dec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f6b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6dd4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "211c1248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a7ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695fa60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36a5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221c9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463cab7-b99c-4e1f-b19e-27aac2cc365a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42c788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a891cd-d4fe-4716-a783-12ac303ea25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpKernel",
   "language": "python",
   "name": "nlpkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
